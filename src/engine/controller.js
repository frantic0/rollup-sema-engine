
// NOTE:FB We need this imports here for webpack to emit this modules from sema-engine package
// import "sema-engine/dist/maximilian.wasmmodule.js";
// import "sema-engine/dist/open303.wasmmodule.js";
// import "sema-engine/dist/maxi-processor.js";
// import "sema-engine/dist/transducers.js";
// import "sema-engine/dist/ringbuf.js";

// import "sema-engine/sema-engine.js";
// import "sema-engine/sema-engine.wasmmodule.js";
// import "sema-engine/open303.wasmmodule.js";
// import "sema-engine/maxi-processor.js";
// import "sema-engine/transducers.js";
// import "sema-engine/ringbuf.js";

// import { PubSub } from "../messaging/pubSub.js";
// import { audioEngineStatus } from "../stores/common.js";
/**
 * The Controller is a singleton class that encapsulates signal engine (sema-engine)
 * and implements the dependency inversion principle
 * @class AudioEngine
 */
export default class Controller {
	/**
	 * @constructor
	 */
	constructor(engine) {
		if (Controller.instance) {
			return Controller.instance; // Singleton pattern, only one instance in sema
		}
		Controller.instance = this;

		// Constructor dependency injection of a sema-engine singleton instance
		this.engine = engine;

		this.samplesLoaded = false;

		// this.messaging = new PubSub();

		// this.messaging.subscribe("eval-dsp", async (e) => {
		// 	this.engine.eval(e); // Also resumes engine playback if paused
		// });

		// this.messaging.subscribe("stop-audio", (e) => this.engine.stop());

		// this.messaging.subscribe("load-sample", (name, url) =>
		// 	this.engine.loadSample(name, url)
		// );

		// this.messaging.subscribe("add-engine-analyser", (e) =>
		// 	this.engine.createAnalyser(e.id, (data) =>
		// 		this.messaging.publish(`${e.id}-analyser-data`, data)
		// 	)
		// );

		// this.messaging.subscribe("remove-engine-analyser", (e) =>
		// 	this.engine.removeAnalyser(e)
		// );

		// this.messaging.subscribe("model-output-data", (e) =>
		// 	this.engine.postAsyncMessageToProcessor(e)
		// );
		// this.messaging.subscribe("clock-phase", (e) =>
		// 	this.engine.postAsyncMessageToProcessor(e)
		// );
		// this.messaging.subscribe("model-send-buffer", (e) =>
		// 	this.engine.postAsyncMessageToProcessor(e)
		// );

		// this.messaging.subscribe("mouse-xy", (e) => {
		// 	this.engine.pushToSharedArrayBuffer("mxy", e);
		// });

		// this.messaging.subscribe("osc", (e) =>
		// 	console.log(`DEBUG:AudioEngine:OSC: ${e}`)
		// );

		//the message has incoming data from other peers
		// this.messaging.subscribe("peermsg", (e) => {
		//   e.ttype = 'NET';
		//   e.peermsg = 1;
		//   this.onMessagingEventHandler(e);
		// });

		// this.messaging.subscribe("peerinfo-request", (e) => {
		// 	console.log(this.peerNet.peerID);
		// 	copyToPasteBuffer(this.peerNet.peerID);
		// });
	}

	/**
	 * Handler of the Pub/Sub message events
	 * whose topics are subscribed to in the audio engine constructor
	 * @onMessagingEventHandler
	 */
	onMessagingEventHandler(event) {
		if (event !== undefined) {
			// Receive notification from "model-output-data" topic
			console.log("DEBUG:AudioEngine:onMessagingEventHandler:");
			console.log(event);
			this.audioWorkletNode.port.postMessage(event);
		}
	}

	/**
	 * Handler of audio worklet processor events
	 * @onProcessorAsyncMessageHandler
	 * @param event
	 */
	onProcessorAsyncMessage(event) {
		if (event !== undefined && event.data !== undefined) {
			// console.log('DEBUG:AudioEngine:processorMessageHandler:');
			// console.log(event);
			// if (event.data.rq !== undefined && event.data.rq === "send") {
			// 	switch (event.data.ttype) {
			// 		case "ML":
			// 			// Stream generated by 'toJS' live code instruction — e.g. {10,0,{1}sin}toJS;
			// 			// publishes to model/JS editor, which posts to ml.worker
			// 			this.messaging.publish("model-input-data", {
			// 				type: "model-input-data",
			// 				value: event.data.value,
			// 				ch: event.data.ch,
			// 			});
			// 			break;
			// 		case "NET":
			// 			this.peerNet.send(
			// 				event.data.ch[0],
			// 				event.data.value,
			// 				event.data.ch[1]
			// 			);
			// 			break;
			// 	}
			// }

      //  else if (event.data.rq && event.data.rq === "buf") {
			// 	console.log("buf", event.data);
			// 	switch (event.data.ttype) {
			// 		case "ML":
			// 			this.messaging.publish("model-input-buffer", {
			// 				type: "model-input-buffer",
			// 				value: event.data.value,
			// 				channelID: event.data.channelID, //channel ID
			// 				blocksize: event.data.blocksize,
			// 			});
			// 			break;
			// 	}
			// }

      // else if (event.data.phase !== undefined) {
				// console.log('DEBUG:AudioEngine:phase:');
				// console.log(event.data.phase);
				// this.kuraClock.broadcastPhase(event.data.phase); // TODO Refactor p to phase
			// }
			// else if (event.data.rq != undefined && event.data.rq === 'receive') {
			//   switch (event.data.ttype) {
			//     case 'ML':
			//       // Stream generated by 'fromJS' live code instruction – e.g. {{10,1}fromJS}saw
			//       // publishes to model/JS editor, which posts to ml.worker
			//       this.messaging.publish('model-output-data-request', {
			//         type: 'model-output-data-request',
			//         value: event.data.value,
			//         channel: event.data.ch
			//       });
			//       break;
			//     case 'NET':
			//       break;
			//   }
			// }
		}
	}

	/**
	 * Initialises audio context and sets worklet processor code
	 * @init
   * @param audioworletURL
   * TODO removing numClockPeers, should be added to a specialised function
	 */
	async init(audioWorkletURL) {
		if (this.engine !== undefined) {
			try {
				await this.engine.init(audioWorkletURL);

				// Subscribe async messages from the Audio Worklet Processor scope
				this.engine.subscribeAsyncMessage(this.onProcessorAsyncMessage);

				// Connect Analysers loaded from the store need to pass callbacks after they load
				this.engine.connectAnalysers();

				const channelId = "mxy",
					ttype = "mouseXY",
					blockSize = 2;

				// Create SharedArrayBuffer for mouse data
				this.engine.createSharedArrayBuffer(channelId, ttype, blockSize);

				// Lazy load all samples imported from assets
				this.loadImportedSamples();
			} catch (error) {
				console.error("Error initialising engine");
			}
		}
	}

	onAudioInputFail(error) {
		console.error(
			`Engine Controller: AudioInputFail – ${error.message} ${error.name}`
		);
	}

	/**
	 * Sets up an AudioIn WAAPI sub-graph
	 * @connectMediaStreamSourceInput
	 */
	async connectMediaStream() {
		const constraints = (window.constraints = {
			audio: true,
			video: false,
		});

		navigator.mediaDevices
			.getUserMedia(constraints)
			.then((s) => this.onAudioInputInit(s))
			.catch(this.onAudioInputFail);
	}

	getSamplesNames() {
		const r = require.context("../../assets/samples", false, /\.wav$/);

		// return an array list of filenames (with extension)
		const importAll = (r) => r.keys().map((file) => file.match(/[^\/]+$/)[0]);

		return importAll(r);
	}

	lazyLoadSample(sampleName) {
		// import(`../../assets/samples/${sampleName}`)
			// .then(() => this.engine.loadSample(sampleName, `/samples/${sampleName}`))
		import(`../../assets/samples/${sampleName}.wav`)
			.then(() => this.engine.loadSample(sampleName, `/samples/${sampleName}`))
			.catch((err) =>
				console.error(`DEBUG:AudioEngine:lazyLoadSample: ` + err)
			);
	}

	loadImportedSamples() {
		this.getSamplesNames().forEach((sampleName) =>
			this.lazyLoadSample(sampleName)
		);

		this.samplesLoaded = true;
	}
}
